# ResNet

Deep Neural Network는 학습 시키기 어렵기 때문에 좀 더 학습 시키기 쉬운 **Residual 학습 방법**을 제안한다.

직전에 나온 VGG보다는 더 깊지만 덜 복잡하다.

레이어만 더 깊게 쌓으면 더 나은 학습을 하는가? 했지만 **vanishing gradients** 문제가 발생

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled.png)

논문에서는 이를 **degradation** 문제라고 말하고 이는 **기울기 소실에 의해 발생**한다고 설명한다.

이 전의 연구들로 모델의 layer가 깊어질수록 오히려 성능이 더 떨어지는 현상이 생겼음을 밝힌 것

**이는 레이어가 깊어질수록 미분을 점점 많이 하기 때문에 back propagation을 해도 앞의 레이어 일 수록 미분 값이 작아져, 그만큼 output에 영향을 끼치는 weight 정도가 작아지는 것을 말한다.**

**(*overfitting은 학습 데이터에 완벽하게 fitting을 시킨 탓에 테스트 성능에서는 안 좋은 결과를 보이는 것)**

위와 같은 문제는 **degradation 문제로, training data에도 학습이 되지 않는 문제**를 뜻한다.

따라서 이를 극복하기 위해 ResNet이 고안된 것

ResNet은 Skip Connection을 이용한 residual learing을 통해 layer가 깊어짐에 따른 vanishing gradient문제를 해결했다.

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%201.png)

기존 방식과의 차이는 x를 더해준 것 밖에 없다.

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%202.png)

input에서 블럭을 거친 output과 초기 input을 더해주는 것

→ activation function을 거친 후 최종 output

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%203.png)

**W1은 Convolution연산**이며 여기에 **activation function(σ)**을 곱하고 다시 **W2인 Convolution 연산**을 취해준다.

따라서 **F**안에 **Wi**는 위와 같이 꼭 2번이 아니라 여러 번 쓸 수 있어서 i로 표현하며

**Ws x** (기존의 입력 값)을 그대로 가져와서 결과에 더해준다

x를 더해주는데 dimention이 같지 않다면 Linear pjojection Ws를 곱하여 맞춰준다.

이러한 식에서 볼 수 있는 Back Propagation 과정에서 위 식을 미분하면,

**y를 x에 대해서 미분하더라도, Ws x 부분이 상수로 남기 때문에 최소 1 이상이므로 곱해질 게 있어서 Vanishing Gradient를 완화 시켜줄 수 있다.**

따라서 기존 VGG보다 layer는 깊지만 복잡도는 적다.

기존 네트워크보다 파라미터 수는 적지만 성능은 더 좋게 나온다.

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%204.png)

위 그림은 ResNet의 구조를 보여주는데,

위의 구조는 VGG-19가 더 깊어진, 34 Layer Plain 네트워크

아래는 34 Layer Residual Network이며 pain network에 short cut connection이 추가되었다.

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%205.png)

이렇게 short cut connection으로 만든 back block을 identity block / convolution block이라고 한다.

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%206.png)

**skip connection**을 통해 이전 정보를 가져와 연산한다.

**Identity Block** 은 네트워크의 output F(x)에 x를 그대로 더하는 것이고

**Convolution Block**은 x도 1*1 convolution 연산을 거친 후 F(x)에 더해주는 것이다.

**왜 Identity Block 과 Convolution Block을 둘 다 더해주는가?**

# ResNet Structure

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%207.png)

### Zero Padding

**zero padding**은 0의 값을 가진 픽셀로 해당 이미지를 둘러싸 주는 것으로, 이렇게 padding을 적용하면 7 * 7 이미지에 3 * 3 필터를 stride 3 으로 적용할 수 있게 된다.

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%208.png)

또한 **padding**을 이용하면 **사이즈를 보존하는 효과**를 얻을 수 있다.

### **padding을 이용해서 사이즈를 보존하는 것이 어떠한 의미를 가지며 왜 중요한가?**

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%209.png)

CNN에서는 Layer가 진행될수록 위의 그림처럼 사이즈가 점점 줄어들게 된다(shrinks volumes).

만약 volume이 빠르게 줄어들어 0이 되어 버리면 더 이상 CNN을 적용할 수 없는 상태가 된다. 이러한 문제를 해결하기 위해 padding을 이용하여 너무 빠르게 사이즈가 줄어들지 않게 보존해주는 것이다.

### Convolution Filter

전체 이미지를 필터로 이동하면서(window sliding) 특징 값을 추출해내는 작업

### Batch Normalization

**ResNet 안에 있는 Batch Normalization은 뭘 하는 건가?**

배치 정규화는 평균과 분산을 조정하는 과정이 별도의 과정으로 떼어진 것이 아니라 신경망 안에 포함되어 학습 시 평균과 분산을 조정하는 과정이다.

각 레이어 마다 정규화 하는 레이어를 두어, 변형된 분포가 나오지 않도록 조절하게 하는 것이 배치 정규화이다.

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%2010.png)

### ReLU

**Rectified Linear Unit 함수**

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%2011.png)

**입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하면 0을 출력하는 활성화 함수**

h(x) = x (x > 0)

  = 0 (x ≤ 0)

Rectified 란 ‘정류된’이라는 뜻으로 +, - 가 반복되는 교류에서 -흐름을 차단하는 회로

```python
def relu(x) :
	return np.maximum(0, x)
```

### Max Pooling

Convolution Block

Identity Block *2

Convolution Block

Identity Block *2

Convolution Block

Identity Block *2

Avg Pooling

### Flatten

2차원 벡터의 행렬을 1차원 배열로 평탄화

### Fully Connected

한 층의 모든 뉴런이 다음 층의 모든 뉴런과 연결된 상태를 말한다.

1차원 배열의 형태로 평탄화된 행렬을 통해 이미지를 분류하는 데 사용되는 계층이다.

위에서는 **Flatten**을 통해 2차원 벡터의 행렬을 1차원 배열로 평탄화 하고,

**ReLU** 함수로 뉴런을 활성화하고,

**SoftMax** 함수로 이미지를 분류하는 것까지가 **Fully Conned Layer**

### 활성화 함수 Activation Function

각 노드마다 값이 존재하고 뉴런마다 가중치와 편향이 존재하여 다른 계층으로  넘어갈 때 각 노드에 가중치를 곱하고 편향을 더한 값이 넘어가게 된다.

그 값이 특정 조건을 만족하게 되면 활성화되었다는 신호를 다음 노드로 보내 활성화하고, 만약 조건을 만족하지 못했으면 해당 노드를 비활성화 하는 함수가 활성화 함수이다.

### 1) ReLU 함수 (Rectified Linear Unit)

### 2) SoftMax 함수

![Untitled](ResNet%209267765583114d1c91ce4a9dde8484a4/Untitled%2012.png)

3개 이상으로 분류하는 다중 클래스 분류에서 사용되는 활성화 함수로,

분류될 클래스가 n개라고 할 때 n차원의 벡터를 입력 받아, **각 클래스에 속할 확률**을 추정한다.

공식은 “ n 번일 확률 / 전체 확률 “

**확률의 총합이 1**이므로, 어떤 분류에 속할 확률이 가장 높을지를 쉽게 인지할 수 있다.

```python
def softmax(x):
	exp_x = np.exp(x)
	result = exp_x / np.sum(exp_x)
	
	return result
```